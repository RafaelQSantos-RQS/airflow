# Production-Ready Airflow Deployment

This project provides a complete, containerized environment for deploying **Apache Airflow**, designed for both robust **local development** and **production** use. It uses a multi-stage `docker-compose` setup and a `Makefile` for a simplified and professional developer experience.

The entire stack is defined as **Infrastructure as Code**, ensuring a repeatable and predictable deployment process.

-----

## üèõÔ∏è Core Architecture

The architecture is built on a multi-file Docker Compose setup to cleanly separate environmental concerns.

* **`docker-compose.yaml`**: This is the **base file** that defines the core, stable architecture of the application, including all required services like the scheduler, worker, and database.
* **`docker-compose.override.yaml`**: This file contains all **local development** configurations. It's automatically loaded by `docker compose` to enable features like building the image locally, mounting local code for hot-reloading, and exposing ports directly to your machine.
* **`docker-compose.prod.yaml`**: This file contains all **production-hardening** configurations. It adds a Traefik reverse proxy, removes direct port access, and sets resource limits for containers.

-----

## Prerequisites

Before you begin, ensure you have the following tools installed on your system:

* Docker and Docker Compose
* `make`
* Git

-----

## üöÄ Quick Start (Local Development)

Getting your local development environment running is a simple, two-step process thanks to the `Makefile` automation.

### 1. Prepare the Environment

First, run the setup command. This will:

* Create a `.env` file from the template.
* Dynamically generate a secure **Fernet Key** and set the correct **Airflow UID**.
* Create the required external Docker network and volume.
* Run the initial database migrations.

```bash
make setup
```

After this step, you should **review and edit** the generated `.env` file with your specific settings.

### 2. Start the Application

Once the setup is complete, start all Airflow services in detached mode. This command will also build your custom Docker image if it's the first time or if the `Dockerfile` has changed.

```bash
make up
```

Your Airflow instance is now running\! You can access the UI at **<http://localhost:8080>**.

-----

## üõ†Ô∏è Makefile Automation

This project uses a `Makefile` to provide a simple, unified interface for all common operations. The main commands are:

| Command | Description |
| :--- | :--- |
| `setup` | Prepares the environment for the first run. |
| `up` | Starts local containers (and builds if necessary). |
| `down` | Stops and removes containers, networks, and volumes. |
| `restart` | Restarts running containers. |
| `rebuild` | Rebuilds images and restarts all services. |
| `logs` | Shows logs in real time. |
| `status` | Shows container status. |
| `deploy` | Deploys the application to production. |

Run `make help` to see all available commands.

-----

## ‚òÅÔ∏è Production Deployment

The production deployment is designed to be run by a CI/CD pipeline and is orchestrated by a single command.

### Workflow

The production deployment uses the `docker-compose.prod.yaml` file to reconfigure the stack for a live environment. This includes:

* Placing the **ApiServer** and **Flower** services behind a Traefik reverse proxy using Docker labels.
* Setting CPU and memory **resource limits** on services to ensure server stability.
* Declaratively scaling the number of **Airflow workers**.

To deploy, you would typically have a `.env.prod` file with your production secrets and run:

```bash
make deploy
```

This command safely orchestrates the entire process: pulling fresh images, running migrations, starting the new containers, and cleaning up old images.

-----

## ‚öôÔ∏è Configuration

All runtime configuration is managed through environment variables.

* **`.env.template`**: This file serves as the template for all required environment variables.
* **`.env`**: This file is for local development. It is generated by the `make setup` command and should **not** be committed to Git.
* **`.env.prod`**: This file should contain your production secrets and configurations.

Key variables include:

* `AIRFLOW_IMAGE_NAME`: The name and tag for your custom Docker image.
* `AIRFLOW_UID`: The user ID for the Airflow process, automatically set to your local user's ID for correct file permissions.
* `AIRFLOW__CORE__FERNET_KEY`: A secret key for encrypting connections, generated automatically by `make setup`.
* `APISERVER_DOMAIN`: The domain name for the Airflow UI, used by the Traefik reverse proxy in production.

-----

## üé® Customization

### Adding Python Dependencies

To add more Python packages to your Airflow environment:

1. Add the package and its version to the **`requirements.txt`** file.
2. Rebuild your Docker image with the command: `make build`.

### Adding DAGs

Simply place your Python DAG files in the `./dags` directory. In the local development environment, the `docker-compose.override.yaml` file mounts this directory into the Airflow containers, allowing for immediate testing of your code changes.
